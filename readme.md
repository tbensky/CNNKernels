# Interpretability of convolutional neural networks via kernel visualizations

The purpose of this work is to do an exercise in the intepretability of a convolutional neural network, by inspecting the evolutions of the kernels during training.

The idea for this work came from a study like [this one](https://towardsdatascience.com/convolution-neural-network-decryption-e323fd18c33), of which there are many.

In sum, we wanted to present a convolutional neural network (CNN) with some simple images, get it all trained, then visualize the resulting kernels.


 